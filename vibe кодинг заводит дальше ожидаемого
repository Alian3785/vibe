# pip install gymnasium stable-baselines3 numpy wandb

import numpy as np
import gymnasium as gym
from gymnasium import spaces
import random
from operator import itemgetter

# ============================================================
#                      –ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï
# ============================================================
UNITS_RED = [
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å1",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 67, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 67, "team": "red",  "position": 1, "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å2",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 33, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 33, "team": "red",  "position": 2, "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å3",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 78, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 78, "team": "red",  "position": 3, "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å7",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 45, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 45, "team": "red",  "position": 4, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å8",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 90, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 90, "team": "red",  "position": 5, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å9",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 12, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 12, "team": "red",  "position": 6, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
]
UNITS_BLUE = [
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å4",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 88, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–∏–≤–∞_–±–∞–∑–∞": 88, "team": "blue", "position": 7,  "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},  # –≤–æ–∑–º–æ–∂–Ω–∞—è –æ–ø–µ—á–∞—Ç–∫–∞
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å5",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 55, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 55, "team": "blue", "position": 8,  "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å6",  "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 22, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 22, "team": "blue", "position": 9,  "stand": "ahead",  "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å10", "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 60, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 60, "team": "blue", "position": 10, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å11", "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 47, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 47, "team": "blue", "position": 11, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
    {"–∏–º—è": "—Ä—ã—Ü–∞—Ä—å12", "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞": 75, "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞": 75, "team": "blue", "position": 12, "stand": "behind", "Type": "Archer", "–£—Ä–æ–Ω": 20, "–ó–¥–æ—Ä–æ–≤—å–µ": 60},
]
# –ü–æ–¥—á–∏—Å—Ç–∏–º –≤–æ–∑–º–æ–∂–Ω—É—é –æ–ø–µ—á–∞—Ç–∫—É –∫–ª—é—á–∞ —É —Å–∏–Ω–µ–≥–æ 4:
for u in UNITS_BLUE:
    if "–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞" not in u and "–∏–Ω–∏—Ü–∏–∞—Ç–∏–∏–≤–∞_–±–∞–∑–∞" in u:
        u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞"] = u.pop("–∏–Ω–∏—Ü–∏–∞—Ç–∏–∏–≤–∞_–±–∞–∑–∞")

BLUE_POSITIONS = list(range(7, 13))
RED_POSITIONS  = list(range(1, 7))
MAX_HP = 60
MAX_INIT = max([u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞"] for u in (UNITS_RED + UNITS_BLUE)])


# ============================================================
#                      –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–†–ï–î–´
# ============================================================

class BattleEnv(gym.Env):
    """
    –ù–∞–±–ª—é–¥–µ–Ω–∏–µ: –≤–µ–∫—Ç–æ—Ä (24,) = [HP1, INI1, ..., HP12, INI12]
    –î–µ–π—Å—Ç–≤–∏–µ: Discrete(6) ‚Äî –≤—ã–±–æ—Ä —Ü–µ–ª–∏ —Å—Ä–µ–¥–∏ –ø–æ–∑–∏—Ü–∏–π RED 1..6
    –ö—Ä–∞—Å–Ω—ã–µ: –±—å—é—Ç –°–õ–£–ß–ê–ô–ù–£–Æ –∂–∏–≤—É—é —Å–∏–Ω—é—é —Ü–µ–ª—å.
    –°–∏–Ω–∏–µ: –∞–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç —Ü–µ–ª—å (—É–¥–∞—Ä –≤ ¬´–º–µ—Ä—Ç–≤—É—é¬ª ‚Äî —Ö–æ–ª–æ—Å—Ç–æ–π).
    –ù–∞–≥—Ä–∞–¥–∞: +1 –ø–æ–±–µ–¥–∞ BLUE, -1 –ø–æ–±–µ–¥–∞ RED, 0 ‚Äî –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ.
    """

    metadata = {"render_modes": []}

    def __init__(self, reward_win: float = 1.0, reward_loss: float = -1.0, reward_step: float = 0.0,
                 log_enabled: bool = False):
        super().__init__()
        self.reward_win = float(reward_win)
        self.reward_loss = float(reward_loss)
        self.reward_step = float(reward_step)
        self.log_enabled = bool(log_enabled)

        self.action_space = spaces.Discrete(6)
        low  = np.zeros(24, dtype=np.float32)
        high = np.array(sum([[MAX_HP, MAX_INIT] for _ in range(12)], []), dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

        self.rng = random.Random()
        self.combined = None
        self.round_no = None
        self.winner = None
        self.current_blue_attacker_pos = None

        self._pretty_events = []  # —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ –ª–æ–≥–∏ (–¥–ª—è —Ç–µ—Å—Ç–∞)

    # --------- —É—Ç–∏–ª–∏—Ç—ã ---------
    def seed(self, seed=None):
        self.rng = random.Random(seed)

    def _alive(self, u): return u["–ó–¥–æ—Ä–æ–≤—å–µ"] > 0
    def _team_alive(self, team): return any(self._alive(u) and u["team"] == team for u in self.combined)
    def _unit_by_position(self, pos): return next((u for u in self.combined if u["position"] == pos), None)
    def _live_positions_of(self, team): return [u["position"] for u in self.combined if u["team"] == team and self._alive(u)]

    def _log(self, s: str):
        if self.log_enabled: self._pretty_events.append(s)

    def pop_pretty_events(self):
        out = self._pretty_events[:]
        self._pretty_events.clear()
        return out

    def _reset_state(self):
        self.combined = [u.copy() for u in (UNITS_RED + UNITS_BLUE)]
        for u in self.combined:
            u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] = u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞"]
        self.round_no = 1
        self.winner = None
        self.current_blue_attacker_pos = None
        self._log(f"–≠–ø–∏–∑–æ–¥ –Ω–∞—á–∞—Ç. –†–∞—É–Ω–¥ {self.round_no}.")

    def _candidates(self):
        return [u for u in self.combined if self._alive(u) and u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] > 0]

    def _pop_next(self):
        cand = self._candidates()
        if not cand:
            return None
        self.rng.shuffle(cand)
        cand.sort(key=itemgetter("–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"), reverse=True)
        return cand[0]

    def _end_round_restore(self):
        for u in self.combined:
            u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] = u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞_–±–∞–∑–∞"] if self._alive(u) else 0
        self._log("–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–Ω–∏—Ü–∏–∞—Ç–∏–≤—ã. –ù–æ–≤—ã–π —Ä–∞—É–Ω–¥.")

    def _attack(self, attacker, target_pos):
        victim = self._unit_by_position(target_pos) if target_pos is not None else None
        if victim is not None and self._alive(victim):
            before = victim["–ó–¥–æ—Ä–æ–≤—å–µ"]
            victim["–ó–¥–æ—Ä–æ–≤—å–µ"] -= attacker["–£—Ä–æ–Ω"]
            after = victim["–ó–¥–æ—Ä–æ–≤—å–µ"]
            self._log(f"{attacker['team'].upper()} {attacker['–∏–º—è']}#{attacker['position']} ‚Üí "
                      f"{victim['team'].upper()} {victim['–∏–º—è']}#{victim['position']}: {attacker['–£—Ä–æ–Ω']} "
                      f"({before}‚Üí{max(0, after)})")
            if victim["–ó–¥–æ—Ä–æ–≤—å–µ"] <= 0:
                victim["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] = 0
                self._log(f"‚úñ {victim['team'].upper()} {victim['–∏–º—è']}#{victim['position']} –≤—ã–≤–µ–¥–µ–Ω –∏–∑ —Å—Ç—Ä–æ—è.")
        else:
            self._log(f"{attacker['team'].upper()} {attacker['–∏–º—è']}#{attacker['position']} –±—å—ë—Ç pos{target_pos}: —Ü–µ–ª–∏ –Ω–µ—Ç/–º–µ—Ä—Ç–≤–∞.")

    def _check_victory_after_hit(self):
        if not self._team_alive("blue"):
            self.winner = "red";  self._log("üèÜ –ü–æ–±–µ–¥–∞ RED!")
        elif not self._team_alive("red"):
            self.winner = "blue"; self._log("üèÜ –ü–æ–±–µ–¥–∞ BLUE!")

    def _advance_until_blue_turn(self):
        while self._team_alive("red") and self._team_alive("blue"):
            nxt = self._pop_next()
            if nxt is None:
                self._log(f"‚Äî –ö–æ–Ω–µ—Ü —Ä–∞—É–Ω–¥–∞ {self.round_no}.")
                self._end_round_restore()
                self.round_no += 1
                self._log(f"‚Äî –ù–∞—á–∞–ª–æ —Ä–∞—É–Ω–¥–∞ {self.round_no}.")
                continue

            if nxt["team"] == "blue":
                self.current_blue_attacker_pos = nxt["position"]
                self._log(f"–•–æ–¥ BLUE: {nxt['–∏–º—è']}#{nxt['position']} (–∏–Ω–∏—Ü {nxt['–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞']}). –û–∂–∏–¥–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏—è.")
                return True

            # –ö—Ä–∞—Å–Ω—ã–π —Ö–æ–¥–∏—Ç: —Å–ª—É—á–∞–π–Ω–∞—è –∂–∏–≤–∞—è —Å–∏–Ω—è—è —Ü–µ–ª—å
            nxt["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] = 0
            live_blue_positions = self._live_positions_of("blue")
            target_pos = self.rng.choice(live_blue_positions) if live_blue_positions else None
            self._log(f"RED —Ö–æ–¥: {nxt['–∏–º—è']}#{nxt['position']} ‚Üí —Å–ª—É—á–∞–π–Ω–∞—è —Ü–µ–ª—å pos{target_pos}.")
            self._attack(nxt, target_pos)
            self._check_victory_after_hit()
            if self.winner is not None:
                return False
        return False

    def _obs(self):
        vec = []
        for pos in RED_POSITIONS + BLUE_POSITIONS:
            u = self._unit_by_position(pos)
            vec.extend([float(max(0, u["–ó–¥–æ—Ä–æ–≤—å–µ"])), float(max(0, u["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"]))])
        return np.array(vec, dtype=np.float32)

    # ----------------- API Gymnasium -----------------
    def reset(self, *, seed=None, options=None):
        if seed is not None:
            self.seed(seed)
        self._reset_state()
        self._advance_until_blue_turn()
        return self._obs(), {}

    def step(self, action):
        assert self.winner is None, "–≠–ø–∏–∑–æ–¥ –∑–∞–≤–µ—Ä—à—ë–Ω ‚Äî –≤—ã–∑–æ–≤–∏—Ç–µ reset()."
        if self.current_blue_attacker_pos is None:
            self._advance_until_blue_turn()
            if self.winner is not None:
                return self._obs(), (self.reward_win if self.winner == "blue" else self.reward_loss), True, False, {}

        # BLUE (–∞–≥–µ–Ω—Ç) –∞—Ç–∞–∫—É–µ—Ç –≤—ã–±—Ä–∞–Ω–Ω—É—é RED-–ø–æ–∑–∏—Ü–∏—é
        target_pos = RED_POSITIONS[int(action)]
        attacker = self._unit_by_position(self.current_blue_attacker_pos)
        if attacker is not None and self._alive(attacker) and attacker["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] > 0:
            self._log(f"BLUE –¥–µ–π—Å—Ç–≤–∏–µ: {attacker['–∏–º—è']}#{attacker['position']} ‚Üí pos{target_pos}")
            attacker["–∏–Ω–∏—Ü–∏–∞—Ç–∏–≤–∞"] = 0
            self._attack(attacker, target_pos)
            self._check_victory_after_hit()

        if self.winner is None:
            self.current_blue_attacker_pos = None
            self._advance_until_blue_turn()

        if self.winner is None:
            reward, terminated = self.reward_step, False
        else:
            reward = self.reward_win if self.winner == "blue" else self.reward_loss
            terminated = True

        return self._obs(), reward, terminated, False, {}


# ============================================================
#                –û–ë–£–ß–ï–ù–ò–ï PPO + W&B –õ–û–ì–ò–†–û–í–ê–ù–ò–ï
# ============================================================
if __name__ == "__main__":
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.monitor import Monitor
    from stable_baselines3.common.callbacks import CallbackList, EvalCallback
    import wandb
    from wandb.integration.sb3 import WandbCallback

    # -------- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è W&B --------
    run = wandb.init(
        project="red-blue-battle",      # ‚Üê –ø–æ–º–µ–Ω—è–π—Ç–µ –Ω–∞ –≤–∞—à –ø—Ä–æ–µ–∫—Ç
        name="ppo-50k",
        config={
            "algo": "PPO",
            "total_timesteps": 50_000,
            "n_envs": 8,
            "n_steps": 1024,
            "batch_size": 2048,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "learning_rate": 3e-4,
            "clip_range": 0.2,
        },
        sync_tensorboard=True,          # ‚Üê —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è TB ‚Üí Weights & Biases
        save_code=True,
    )

    # –û–±—ë—Ä—Ç–∫–∞-—Ñ–∞–±—Ä–∏–∫–∞ —Å—Ä–µ–¥—ã: –±–µ–∑ –ª–æ–≥–æ–≤ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    def make_env():
        # Monitor –Ω—É–∂–µ–Ω, —á—Ç–æ–±—ã SB3 –ø–∏—Å–∞–ª —ç–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ (ep_rew_mean)
        return Monitor(BattleEnv(reward_win=1.0, reward_loss=-1.0, reward_step=0.0, log_enabled=False))

    # –í–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã + Monitor
    vec_env = make_vec_env(make_env, n_envs=8, seed=42, monitor_dir="./monitor")

    # –û—Ç–¥–µ–ª—å–Ω–∞—è eval-—Å—Ä–µ–¥–∞ –¥–ª—è –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ (–º–µ—Ç—Ä–∏–∫–∏ —É–π–¥—É—Ç –≤ TB ‚Üí W&B)
    eval_env = Monitor(BattleEnv(log_enabled=False))

    model = PPO(
        policy="MlpPolicy",
        env=vec_env,
        verbose=1,                    # —ç—Ç–æ –ª–æ–≥ SB3
        n_steps=1024,
        batch_size=2048,
        gae_lambda=0.95,
        gamma=0.99,
        n_epochs=10,
        learning_rate=3e-4,
        clip_range=0.2,
        ent_coef=0.0,
        vf_coef=0.5,
        seed=42,
        tensorboard_log=f"./tb_logs/{run.id}",  # ‚Üê —á—Ç–æ–±—ã TB-—Å–∫–∞–ª—è—Ä—ã —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–ª–∏—Å—å –≤ W&B
    )

    # W&B callback + –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞
    wandb_cb = WandbCallback(
        gradient_save_freq=1000,                   # –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å/—É–≤–µ–ª–∏—á–∏—Ç—å
        model_save_path=f"./models/{run.id}",      # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á–µ–∫–ø–æ–∏–Ω—Ç—ã
        model_save_freq=10_000,                    # —á–∞—Å—Ç–æ—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏
        verbose=2,
    )
    eval_cb = EvalCallback(
        eval_env,
        best_model_save_path=f"./models/{run.id}/best",
        log_path=f"./eval/{run.id}",
        eval_freq=1000,                # –∫–∞–∂–¥—ã–µ 10k —à–∞–≥–æ–≤ ‚Äî  –æ—Ü–µ–Ω–∫–∞
        n_eval_episodes=5,
        deterministic=True,
        render=False,
    )
    callbacks = CallbackList([wandb_cb, eval_cb])

    # -------- –û–±—É—á–µ–Ω–∏–µ —Å —Ç—Ä–µ–∫–∏–Ω–≥–æ–º –≤ W&B --------
    model.learn(total_timesteps=500000, callback=callbacks)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
    model.save("ppo_blue_vs_red")
    run.finish()  # –∑–∞–≤–µ—Ä—à–∏—Ç—å W&B-–∑–∞–ø—É—Å–∫

    # =======================================================
    #   –¢–ï–°–¢–û–í–´–ô –ü–†–û–ì–û–ù –° –ß–ò–¢–ê–ï–ú–´–ú–ò –õ–û–ì–ê–ú–ò (1 —ç–ø–∏–∑–æ–¥)
    # =======================================================
    print("\n=== –¢–ï–°–¢–û–í–´–ô –ü–†–û–ì–û–ù –° –õ–û–ì–ê–ú–ò ===")
    test_env = BattleEnv(log_enabled=True)   # –≤–∫–ª—é—á–∞–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º—ã–µ –ª–æ–≥–∏ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–∞
    obs, info = test_env.reset(seed=123)

    # –ø–µ—á–∞—Ç–∞–µ–º –ª–æ–≥–∏ –∞–≤—Ç–æ–¥–æ–∫—Ä—É—Ç–∫–∏ –¥–æ —Ö–æ–¥–∞ BLUE
    for line in test_env.pop_pretty_events():
        print(line)

    done = False
    total_reward = 0.0
    step_i = 0
    while not done:
        step_i += 1
        action, _ = model.predict(obs, deterministic=True)
        target_pos = RED_POSITIONS[int(action)]
        print(f"\n[STEP {step_i}] –ê–≥–µ–Ω—Ç –≤—ã–±–∏—Ä–∞–µ—Ç action={int(action)} ‚Üí –∞—Ç–∞–∫–∞ RED pos{target_pos}")

        obs, reward, terminated, truncated, info = test_env.step(action)
        total_reward += reward
        done = terminated or truncated

        for line in test_env.pop_pretty_events():
            print(line)

    print("\n=== –≠–ü–ò–ó–û–î –ó–ê–í–ï–†–®–Å–ù ===")
    print("–ü–æ–±–µ–¥–∏—Ç–µ–ª—å:", test_env.winner.upper(), "| –°—É–º–º–∞—Ä–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞:", total_reward)
