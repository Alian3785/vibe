# ======================== ИМПОРТЫ ===========================
# Библиотеки стандартной библиотеки Python
import os
import re
import time
import shutil
import random
from operator import itemgetter

# Научный стек
import numpy as np

# Gymnasium (современная ветка OpenAI Gym)
import gymnasium as gym
from gymnasium import spaces

# ====================== ИСХОДНЫЕ ДАННЫЕ =====================
# В этом блоке определяем состав двух команд и константы боя.
# По условию 12 юнитов: RED (позиции 1..6), BLUE (позиции 7..12).
# Все характеристики фиксированы: Урон=20, Здоровье=60.
# Инициатива определяет порядок хода в раунде.

UNITS_RED = [
    {"имя": "рыцарь1",  "инициатива": 67, "инициатиива_база": 67, "team": "red",  "position": 1, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь2",  "инициатива": 33, "инициатиива_база": 33, "team": "red",  "position": 2, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь3",  "инициатива": 78, "инициатиива_база": 78, "team": "red",  "position": 3, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь7",  "инициатива": 45, "инициатиива_база": 45, "team": "red",  "position": 4, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь8",  "инициатива": 90, "инициатиива_база": 90, "team": "red",  "position": 5, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь9",  "инициатива": 12, "инициатиива_база": 12, "team": "red",  "position": 6, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
]

# На всякий случай унифицируем ключ базовой инициативы у RED (если где-то опечатались)
for u in UNITS_RED:
    if "инициатива_база" not in u and "инициатиива_база" in u:
        u["инициатива_база"] = u.pop("инициатиива_база")

UNITS_BLUE = [
    {"имя": "рыцарь4",  "инициатива": 88, "инициатива_база": 88, "team": "blue", "position": 7,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь5",  "инициатива": 55, "инициатива_база": 55, "team": "blue", "position": 8,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь6",  "инициатива": 22, "инициатива_база": 22, "team": "blue", "position": 9,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь10", "инициатива": 60, "инициатива_база": 60, "team": "blue", "position": 10, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь11", "инициатива": 47, "инициатива_база": 47, "team": "blue", "position": 11, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь12", "инициатива": 75, "инициатива_база": 75, "team": "blue", "position": 12, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
]

# Два диапазона позиций: RED (1..6) и BLUE (7..12)
RED_POSITIONS  = list(range(1, 7))
BLUE_POSITIONS = list(range(7, 13))

# Константы характеристик
MAX_HP   = 60
MAX_INIT = max([u["инициатива_база"] for u in (UNITS_RED + UNITS_BLUE)])

# ==================== КЛАСС СРЕДЫ GYMNASIUM ==================
class BattleEnv(gym.Env):
    """
    КАСТОМНАЯ СРЕДА «RED vs BLUE» ДЛЯ RL

    Наблюдение (obs):
        Вектор длины 24 (float32) = по 2 признака на каждую из 12 позиций:
        [HP1, INI1, HP2, INI2, ..., HP12, INI12]
        Порядок позиций фиксированный: сначала 1..6 (RED), затем 7..12 (BLUE).

    Действие (action):
        Discrete(6) — индекс 0..5 -> удар по позиции RED_POSITIONS[action], т.е. по позициям 1..6.

    Порядок ходов:
        — Внутри раунда ходят по инициативе (чем выше, тем раньше).
        — Как только юнит сходил в текущем раунде, его инициатива обнуляется (в этом раунде он больше не ходит).
        — Когда у всех живых инициатива = 0, начинается новый раунд, и живым инициатива восстанавливается до базовой.

    Логика сторон:
        — BLUE (агент): когда по инициативе доходит очередь живого синего юнита, среда «останавливается»,
          возвращает наблюдение и ждёт действие агента (выбор RED-цели). Затем применяет удар и проверяет победу.
        — RED (противник): на своём ходу выбирает СЛУЧАЙНУЮ живую цель среди BLUE и атакует её.

    Завершение эпизода:
        — Сразу после каждого удара проверяется, остались ли живые у обеих сторон.
        — Как только у стороны не осталось живых — эпизод терминален (terminated=True).

    Вознаграждение:
        — В процессе: 0 (по умолчанию).
        — Победа BLUE: +1.
        — Победа RED : -1.
    """

    metadata = {"render_modes": []}

    def __init__(self, reward_win: float = 1.0, reward_loss: float = -1.0, reward_step: float = 0.0,
                 log_enabled: bool = False):
        """
        :param reward_win:   награда за победу BLUE
        :param reward_loss:  награда за поражение BLUE
        :param reward_step:  промежуточная награда за шаг (shaping), по умолчанию 0
        :param log_enabled:  включить человекочитаемые логи (используем только на тесте)
        """
        super().__init__()
        # Параметры наград
        self.reward_win  = float(reward_win)
        self.reward_loss = float(reward_loss)
        self.reward_step = float(reward_step)

        # Флаг лога для человекочитаемых сообщений (только для тестового прогона)
        self.log_enabled = bool(log_enabled)

        # Пространство действий: выбор целевой позиции среди 6 красных слотов
        self.action_space = spaces.Discrete(6)

        # Пространство наблюдений: 24 float32 (HP и INI на 12 позициях)
        low  = np.zeros(24, dtype=np.float32)
        high = np.array(sum([[MAX_HP, MAX_INIT] for _ in range(12)], []), dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

        # ГСЧ для воспроизводимости (инициализируем через seed())
        self.rng = random.Random()

        # Текущее состояние боя
        self.combined = None                   # список из 12 dict-юнитов (копии исходных)
        self.round_no = None                   # номер текущего раунда
        self.winner = None                     # "red" | "blue" | None
        self.current_blue_attacker_pos = None  # позиция синего, который ждёт действие агента

        # Буфер человекочитаемых логов (строки), используем только если log_enabled=True
        self._pretty_events = []

    # ------------------ ВСПОМОГАТЕЛЬНЫЕ МЕТОДЫ ------------------

    def seed(self, seed=None):
        """Установить seed для генератора случайных чисел (воспроизводимость)."""
        self.rng = random.Random(seed)

    def _alive(self, u) -> bool:
        """Вернуть True, если юнит жив (HP > 0)."""
        return u["Здоровье"] > 0

    def _team_alive(self, team: str) -> bool:
        """Есть ли у команды team хотя бы один живой юнит."""
        return any(self._alive(u) and u["team"] == team for u in self.combined)

    def _unit_by_position(self, pos: int):
        """Вернуть словарь-юнит по позиции pos (1..12), либо None, если пусто."""
        return next((u for u in self.combined if u["position"] == pos), None)

    def _live_positions_of(self, team: str):
        """Вернуть список позиций живых юнитов указанной команды."""
        return [u["position"] for u in self.combined if u["team"] == team and self._alive(u)]

    def _log(self, s: str):
        """Добавить строку в человекочитаемый лог (используется при тестовом прогоне)."""
        if self.log_enabled:
            self._pretty_events.append(s)

    def pop_pretty_events(self):
        """
        Забрать накопленные логи и очистить буфер.
        Удобно вызывать сразу после reset()/step(), чтобы получить «кадр» логов.
        """
        out = self._pretty_events[:]
        self._pretty_events.clear()
        return out

    def _reset_state(self):
        """
        Полная инициализация состояния эпизода:
        — Клонируем исходные словари юнитов (чтобы не мутировать глобальные).
        — Устанавливаем текущую инициативу равной базовой.
        — Сбрасываем флаги: номер раунда, победитель, «ожидающий BLUE».
        """
        # У RED заранее нормализовали ключи, у BLUE всё ок
        self.combined = [u.copy() for u in (UNITS_RED + UNITS_BLUE)]
        for u in self.combined:
            u["инициатива"] = u["инициатива_база"]
        self.round_no = 1
        self.winner = None
        self.current_blue_attacker_pos = None
        self._log(f"Эпизод начат. Раунд {self.round_no}.")

    def _candidates(self):
        """
        Вернуть список юнитов, которые могут ходить в текущем раунде:
        — юнит должен быть жив, и
        — его текущая инициатива должна быть > 0.
        """
        return [u for u in self.combined if self._alive(u) and u["инициатива"] > 0]

    def _pop_next(self):
        """
        Выбор следующего ходящего:
        1) Берём кандидатов (живые, INI>0).
        2) Перемешиваем (решаем ничьи по инициативе случайно).
        3) Сортируем по инициативе по убыванию.
        4) Возвращаем верхнего.
        Если кандидатов нет — возвращаем None (конец раунда).
        """
        cand = self._candidates()
        if not cand:
            return None
        self.rng.shuffle(cand)
        cand.sort(key=itemgetter("инициатива"), reverse=True)
        return cand[0]

    def _end_round_restore(self):
        """
        Конец раунда: всем ЖИВЫМ возвращаем инициативу к базовой (инициатива_база),
        мёртвым оставляем 0. Увеличение номера раунда происходит снаружи.
        """
        for u in self.combined:
            u["инициатива"] = u["инициатива_база"] if self._alive(u) else 0
        self._log("Восстановление инициативы. Новый раунд.")

    # ------------------- УДАР И ПРОВЕРКА ПОБЕДЫ -------------------

    def _attack(self, attacker, target_pos: int):
        """
        Выполнить атаку «attacker» по позиции target_pos:
        — Если в целевой позиции есть живой юнит, отнимаем у него HP на величину Урон (20).
        — Если после удара HP <= 0, сбрасываем инициативу жертвы (в этом раунде она уже не сходится).
        — Пишем человекочитаемую запись (если включён лог).
        """
        victim = self._unit_by_position(target_pos) if target_pos is not None else None
        if victim is not None and self._alive(victim):
            before = victim["Здоровье"]
            victim["Здоровье"] -= attacker["Урон"]
            after = victim["Здоровье"]
            self._log(f"{attacker['team'].upper()} {attacker['имя']}#{attacker['position']} → "
                      f"{victim['team'].upper()} {victim['имя']}#{victim['position']}: {attacker['Урон']} "
                      f"({before}→{max(0, after)})")
            if victim["Здоровье"] <= 0:
                victim["инициатива"] = 0
                self._log(f"✖ {victim['team'].upper()} {victim['имя']}#{victim['position']} выведен из строя.")
        else:
            self._log(f"{attacker['team'].upper()} {attacker['имя']}#{attacker['position']} бьёт pos{target_pos}: цели нет/мертва.")

    def _check_victory_after_hit(self):
        """
        Сразу после любого удара проверяем, остались ли живые у обеих сторон.
        При отсутствии живых у одной из сторон — фиксируем победителя и логируем событие.
        """
        if not self._team_alive("blue"):
            self.winner = "red"
            self._log("🏆 Победа RED!")
        elif not self._team_alive("red"):
            self.winner = "blue"
            self._log("🏆 Победа BLUE!")

    # --------------- АВТОДОКРУТКА ДО ХОДА BLUE ----------------

    def _advance_until_blue_turn(self) -> bool:
        """
        Прокручиваем бой автоматически, пока не наступит очередь живого BLUE,
        или пока бой не завершится.
        Возвращает:
            True  — если остановились на ходу BLUE (среда ждёт действие агента),
            False — если бой завершился до этого момента.
        Внутри:
          — Если нет кандидатов на ход → конец раунда: восстановить инициативу и начать новый.
          — Если ход RED → выбрать случайную живую цель среди BLUE и атаковать.
          — После каждого удара — проверка победителя.
        """
        while self._team_alive("red") and self._team_alive("blue"):
            nxt = self._pop_next()
            if nxt is None:
                # Все живые уже сходили — начинаем новый раунд
                self._log(f"— Конец раунда {self.round_no}.")
                self._end_round_restore()
                self.round_no += 1
                self._log(f"— Начало раунда {self.round_no}.")
                continue

            if nxt["team"] == "blue":
                # Пришёл ход живого BLUE — выходим и ждём action от агента
                self.current_blue_attacker_pos = nxt["position"]
                self._log(f"Ход BLUE: {nxt['имя']}#{nxt['position']} (иниц {nxt['инициатива']}). Ожидание действия.")
                return True

            # === Ход RED (противник): случайный выбор живой цели среди BLUE ===
            nxt["инициатива"] = 0  # в рамках текущего раунда этот юнит сходил
            live_blue_positions = self._live_positions_of("blue")
            target_pos = self.rng.choice(live_blue_positions) if live_blue_positions else None
            self._log(f"RED ход: {nxt['имя']}#{nxt['position']} → случайная цель pos{target_pos}.")
            self._attack(nxt, target_pos)
            self._check_victory_after_hit()
            if self.winner is not None:
                return False

        # Боевые действия закончились до очереди BLUE
        return False

    # -------------------- НАБЛЮДЕНИЕ ДЛЯ АГЕНТА --------------------

    def _obs(self) -> np.ndarray:
        """
        Построить наблюдение: плоский вектор (24,) = [HP, INI] для pos=1..12.
        Отрицательные значения (не должны появляться) обрезаем до нуля для стабильности.
        """
        vec = []
        for pos in RED_POSITIONS + BLUE_POSITIONS:
            u = self._unit_by_position(pos)
            vec.extend([float(max(0, u["Здоровье"])), float(max(0, u["инициатива"]))])
        return np.array(vec, dtype=np.float32)

    # ------------------------ API GYMNASIUM ------------------------

    def reset(self, *, seed=None, options=None):
        """
        Сброс эпизода:
          1) Инициализируем состояние боя.
          2) Автоматически докручиваем до первого хода BLUE (или терминала).
          3) Возвращаем начальное наблюдение и пустой info (по стандарту Gymnasium).
        """
        if seed is not None:
            self.seed(seed)
        self._reset_state()
        self._advance_until_blue_turn()
        return self._obs(), {}

    def step(self, action):
        """
        Один шаг среды:
          — Если вдруг сейчас не очередь BLUE, докручиваем до BLUE (или завершаем, если бой уже закончился).
          — Когда очередь BLUE: берём позицию атакующего BLUE и применяем действие агента:
                target_pos = RED_POSITIONS[action]  (удар по одной из 6 красных позиций)
            После удара сразу проверяем победителя.
          — Если победителя нет — снова докручиваем до следующего BLUE (между делом ходят RED по своей политике).
          — Возвращаем (obs, reward, terminated, truncated, info).
        Награда:
          — 0, если бой продолжается;
          — +1, если победил BLUE;
          — -1, если победил RED.
        """
        # Если эпизод уже завершён — просим вызвать reset()
        assert self.winner is None, "Эпизод завершён — вызовите reset()."

        # Подстраховка: если не очередь BLUE, докрутим до его хода
        if self.current_blue_attacker_pos is None:
            self._advance_until_blue_turn()
            if self.winner is not None:
                # Бой завершился ещё до хода BLUE
                return self._obs(), (self.reward_win if self.winner == "blue" else self.reward_loss), True, False, {}

        # === ХОД BLUE (агент) ===
        target_pos = RED_POSITIONS[int(action)]          # перевести индекс действия в реальную позицию
        attacker   = self._unit_by_position(self.current_blue_attacker_pos)
        if attacker is not None and self._alive(attacker) and attacker["инициатива"] > 0:
            self._log(f"BLUE действие: {attacker['имя']}#{attacker['position']} → pos{target_pos}")
            attacker["инициатива"] = 0                   # в этом раунде синий уже сходил
            self._attack(attacker, target_pos)          # применяем урон
            self._check_victory_after_hit()             # проверяем победителя

        # Если бой не завершён — докрутка до следующего BLUE (RED будет ходить автоматически)
        if self.winner is None:
            self.current_blue_attacker_pos = None
            self._advance_until_blue_turn()

        # Финальная сборка перехода
        if self.winner is None:
            reward, terminated = self.reward_step, False
        else:
            reward  = self.reward_win if self.winner == "blue" else self.reward_loss
            terminated = True

        return self._obs(), reward, terminated, False, {}

# =================== ОБУЧЕНИЕ PPO + W&B =====================
if __name__ == "__main__":
    # Импортируем Stable-Baselines3 и колбэки прямо внутри main-блока (чтобы при импорте модуля они не требовались)
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.monitor import Monitor
    from stable_baselines3.common.callbacks import CallbackList, EvalCallback

    # Weights & Biases для трекинга обучения
    import wandb
    from wandb.integration.sb3 import WandbCallback

    # ---------------- ПАРАМЕТРЫ ОБУЧЕНИЯ ----------------
    TOTAL_STEPS   = 50_000   # сколько шагов среды сделает PPO (суммарно по всем векторным копиям)
    N_ENVS        = 8        # сколько параллельных копий среды использовать
    VISUALIZE_TEST = True    # включить ли опциональную визуализацию после теста
    FRAME_DELAY    = 0.28    # задержка между «кадрами» визуализации (сек)
    USE_COLOR      = True    # цветные ANSI в консоли (если терминал поддерживает)

    # ------------- ИНИЦИАЛИЗАЦИЯ W&B ЛОГА --------------
    # Перед запуском сделайте wandb.login() один раз в системе/сессии.
    run = wandb.init(
        project="red-blue-battle",           # поменяйте на ваш проект в W&B
        name=f"ppo-{TOTAL_STEPS//1000}k",    # имя запуска
        config={                             # полезные гиперпараметры для карточки W&B
            "algo": "PPO",
            "total_timesteps": TOTAL_STEPS,
            "n_envs": N_ENVS,
            "n_steps": 1024,
            "batch_size": 2048,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "learning_rate": 3e-4,
            "clip_range": 0.2,
        },
        sync_tensorboard=True,               # синхронизация скаляров TensorBoard → W&B
        save_code=True,                      # прикрепить код к запуску
    )

    # -------- ФАБРИКА СРЕД ДЛЯ ВЕКТОРИЗАЦИИ --------
    def make_env():
        # Оборачиваем среду в Monitor, чтобы SB3 считал эпизодические метрики (ep_rew_mean и т.п.)
        return Monitor(BattleEnv(reward_win=1.0, reward_loss=-1.0, reward_step=0.0, log_enabled=False))

    # Векторная среда: N_ENVS параллельных копий для ускоренного сбора опыта
    vec_env = make_vec_env(make_env, n_envs=N_ENVS, seed=42)

    # Отдельная среда для периодической оценки (EvalCallback)
    eval_env = Monitor(BattleEnv(log_enabled=False))

    # --------------- НАСТРОЙКА МОДЕЛИ PPO ---------------
    model = PPO(
        policy="MlpPolicy",
        env=vec_env,
        verbose=1,                         # печать прогресса обучения SB3
        n_steps=1024,                      # длина rollout на КАЖДУЮ среду (итого N_ENVS * n_steps шагов на обновление)
        batch_size=2048,                   # размер мини-батча SGD (должен делиться на N_ENVS*n_steps / n_epochs)
        gae_lambda=0.95,
        gamma=0.99,
        n_epochs=10,
        learning_rate=3e-4,
        clip_range=0.2,
        ent_coef=0.0,
        vf_coef=0.5,
        seed=42,
        tensorboard_log=f"./tb_logs/{run.id}",  # куда писать TB-логи (W&B их подхватит)
    )

    # Колбэк W&B: сохранение градиентов/модели и синк метрик
    wandb_cb = WandbCallback(
        gradient_save_freq=1000,                     # как часто логировать градиенты (можно убрать для скорости)
        model_save_path=f"./models/{run.id}",        # куда сохранять чекпоинты
        model_save_freq=10_000,                      # период сохранения модели
        verbose=2,
    )

    # Колбэк оценки: периодически замеряем среднюю награду на eval_env
    eval_cb = EvalCallback(
        eval_env,
        best_model_save_path=f"./models/{run.id}/best",
        log_path=f"./eval/{run.id}",
        eval_freq=10_000,               # каждые 10k шагов
        n_eval_episodes=5,
        deterministic=True,
        render=False,
    )

    # Запуск обучения с колбэками (W&B + eval)
    model.learn(total_timesteps=TOTAL_STEPS, callback=CallbackList([wandb_cb, eval_cb]))

    # Сохранение финальной модели
    model.save("ppo_blue_vs_red")
    run.finish()  # корректно закрываем W&B-запуск

    # ================= ТЕСТОВЫЙ ПРОГОН С ЛОГАМИ =================
    print("\n=== ТЕСТОВЫЙ ПРОГОН С ЛОГАМИ ===")
    test_env = BattleEnv(log_enabled=True)      # включаем человекочитаемые логи только на тесте
    obs, info = test_env.reset(seed=123)

    # Собираем все строки логов (будут использованы и для опциональной визуализации)
    all_logs = []
    all_logs += test_env.pop_pretty_events()    # логи, накопленные во время reset()/автодокрутки

    done = False
    total_reward = 0.0
    step_i = 0
    while not done:
        step_i += 1
        action, _ = model.predict(obs, deterministic=True)  # в тесте используем детерминированную политику
        target_pos = RED_POSITIONS[int(action)]
        chosen_line = f"[STEP {step_i}] Агент выбирает action={int(action)} → атака RED pos{target_pos}"
        print("\n" + chosen_line)
        all_logs.append(chosen_line)

        obs, reward, terminated, truncated, info = test_env.step(action)
        total_reward += reward
        done = terminated or truncated

        # Печатаем логи текущего шага и добавляем их в общий список
        new_lines = test_env.pop_pretty_events()
        all_logs += new_lines
        for line in new_lines:
            print(line)

    print("\n=== ЭПИЗОД ЗАВЕРШЁН ===")
    print("Победитель:", test_env.winner.upper(), "| Суммарная награда:", total_reward)

    # ============== ОПЦИОНАЛЬНАЯ ВИЗУАЛИЗАЦИЯ ПО ЛОГАМ ==============
    # Визуализация полностью отделена от логики и строится ТОЛЬКО по строкам логов.
    if VISUALIZE_TEST:
        # ANSI-цвета для красоты (можно отключить через USE_COLOR=False)
        ANSI = {
            "reset": "\033[0m",
            "bold": "\033[1m",
            "red": "\033[31m",
            "blue": "\033[34m",
            "yellow": "\033[33m",
        }

        def colorize(s, c=None):
            """Обернуть строку ANSI-кодом цвета (если разрешено)."""
            if not USE_COLOR or c not in ANSI:
                return s
            return f"{ANSI[c]}{s}{ANSI['reset']}"

        def clear_console():
            """Очистить консоль к следующему «кадру»."""
            os.system("cls" if os.name == "nt" else "clear")

        def hp_bar(hp, max_hp=MAX_HP, width=14):
            """Построить текстовую полоску HP фиксированной ширины."""
            hp = max(0, min(hp, max_hp))
            filled = int(round((hp / max_hp) * width))
            return "█" * filled + "░" * (width - filled)

        # Состояние для визуализации восстанавливаем ТОЛЬКО из логов:
        # стартовые HP у всех = 60, дальше обновляем по строкам «(before→after)».
        state = {u["position"]: {"team": u["team"], "name": u["имя"], "hp": MAX_HP}
                 for u in (UNITS_RED + UNITS_BLUE)}

        # Регулярки для вытаскивания фактов из человекочитаемых строк логов
        # Пример строки удара: "RED Имя#pos → BLUE Имя#pos: 20 (60→40)"
        atk_re = re.compile(
            r'^(RED|BLUE)\s+([^\s#]+)#(\d+)\s+→\s+(RED|BLUE)\s+([^\s#]+)#(\d+):\s+(\d+)\s+\((\d+)→(\d+)\)'
        )
        # Пример строки убийства: "✖ RED Имя#pos выведен из строя."
        kill_re = re.compile(r'^✖\s+(RED|BLUE)\s+([^\s#]+)#(\d+)\s+выведен из строя\.')
        # Пример строки победы: "🏆 Победа RED!"
        victory_re = re.compile(r'^🏆 Победа (RED|BLUE)!')

        def board_snapshot():
            """Сделать снимок состояния поля: слева RED 1..6, справа BLUE 7..12."""
            left = [state[p] for p in RED_POSITIONS]
            right = [state[p] for p in BLUE_POSITIONS]
            header = colorize("   RED (1–6)".ljust(50) + "BLUE (7–12)", "bold")
            lines = [header]
            for i in range(6):
                lu = left[i]; ru = right[i]
                ltxt = f"{colorize(lu['name'][:10].ljust(10), 'red')}  [{hp_bar(lu['hp'])}] {str(lu['hp']).rjust(3)}hp"
                rtxt = f"{colorize(ru['name'][:10].ljust(10), 'blue')} [{hp_bar(ru['hp'])}] {str(ru['hp']).rjust(3)}hp"
                lines.append(f"{ltxt:<50} {rtxt}")
            return "\n".join(lines)

        def render_frame(banner):
            """Отрисовать один «кадр» визуализации: поле + подпись события."""
            clear_console()
            term_w = shutil.get_terminal_size((100, 25)).columns
            sep = "─" * min(term_w, 100)
            print(board_snapshot())
            print("\n" + sep)
            print(banner)
            print(sep)

        # Пошагово проигрываем ленты логов, обновляя HP по значениям «after» из строк ударов
        for line in all_logs:
            # Если это строка удара — обновляем HP жертвы до «after»
            m = atk_re.match(line)
            if m:
                vic_pos = int(m.group(6))   # позиция жертвы
                after  = int(m.group(9))    # HP после удара
                if vic_pos in state:
                    state[vic_pos]["hp"] = after
                render_frame(colorize(line, "yellow"))
                time.sleep(FRAME_DELAY)
                continue

            # Если это убийство/победа/служебная строка — просто показываем кадр
            if kill_re.match(line) or victory_re.match(line) or line.startswith("— ") or "Ход BLUE" in line or "RED ход" in line or line.startswith("[STEP"):
                render_frame(colorize(line, "yellow"))
                time.sleep(FRAME_DELAY / 1.2)

        # Небольшая пауза в конце анимации
        time.sleep(1.2)
