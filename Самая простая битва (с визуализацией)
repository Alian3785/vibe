# ======================== ИМПОРТЫ ===========================
# Библиотеки стандартной библиотеки Python
import os
import re
import time
import shutil
import random
from operator import itemgetter

# Научный стек
import numpy as np

# Gymnasium (современная ветка OpenAI Gym)
import gymnasium as gym
from gymnasium import spaces

# ====================== ИСХОДНЫЕ ДАННЫЕ =====================
# В этом блоке определяем состав двух команд и константы боя.
# По условию 12 юнитов: RED (позиции 1..6), BLUE (позиции 7..12).
# Все характеристики фиксированы: Урон=20, Здоровье=60.
# Инициатива определяет порядок хода в раунде.

UNITS_RED = [
    {"имя": "рыцарь1",  "инициатива": 67, "инициатива_база": 67, "team": "red",  "position": 1, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь2",  "инициатива": 33, "инициатива_база": 33, "team": "red",  "position": 2, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь3",  "инициатива": 78, "инициатива_база": 78, "team": "red",  "position": 3, "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь7",  "инициатива": 45, "инициатива_база": 45, "team": "red",  "position": 4, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь8",  "инициатива": 90, "инициатива_база": 90, "team": "red",  "position": 5, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь9",  "инициатива": 12, "инициатива_база": 12, "team": "red",  "position": 6, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
]


UNITS_BLUE = [
    {"имя": "рыцарь4",  "инициатива": 88, "инициатива_база": 88, "team": "blue", "position": 7,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь5",  "инициатива": 55, "инициатива_база": 55, "team": "blue", "position": 8,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь6",  "инициатива": 22, "инициатива_база": 22, "team": "blue", "position": 9,  "stand": "ahead",  "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь10", "инициатива": 60, "инициатива_база": 60, "team": "blue", "position": 10, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь11", "инициатива": 47, "инициатива_база": 47, "team": "blue", "position": 11, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
    {"имя": "рыцарь12", "инициатива": 75, "инициатива_база": 75, "team": "blue", "position": 12, "stand": "behind", "Type": "Archer", "Урон": 20, "Здоровье": 60},
]

# Два диапазона позиций: RED (1..6) и BLUE (7..12)
RED_POSITIONS  = list(range(1, 7))
BLUE_POSITIONS = list(range(7, 13))

# Константы характеристик
MAX_HP   = 60
MAX_INIT = max([u["инициатива_база"] for u in (UNITS_RED + UNITS_BLUE)])

# ==================== КЛАСС СРЕДЫ GYMNASIUM ==================
class BattleEnv(gym.Env):
    """
    КАСТОМНАЯ СРЕДА «RED vs BLUE» ДЛЯ RL

    Наблюдение (obs):
        Вектор длины 24 (float32) = по 2 признака на каждую из 12 позиций:
        [HP1, INI1, HP2, INI2, ..., HP12, INI12]
        Порядок позиций фиксированный: сначала 1..6 (RED), затем 7..12 (BLUE).

    Действие (action):
        Discrete(6) — индекс 0..5 -> удар по позиции RED_POSITIONS[action], т.е. по позициям 1..6.

    Порядок ходов:
        — Внутри раунда ходят по инициативе (чем выше, тем раньше).
        — Как только юнит сходил в текущем раунде, его инициатива обнуляется (в этом раунде он больше не ходит).
        — Когда у всех живых инициатива = 0, начинается новый раунд, и живым инициатива восстанавливается до базовой.

    Логика сторон:
        — BLUE (агент): когда по инициативе доходит очередь живого синего юнита, среда «останавливается»,
          возвращает наблюдение и ждёт действие агента (выбор RED-цели). Затем применяет удар и проверяет победу.
        — RED (противник): на своём ходу выбирает СЛУЧАЙНУЮ живую цель среди BLUE и атакует её.

    Завершение эпизода:
        — Сразу после каждого удара проверяется, остались ли живые у обеих сторон.
        — Как только у стороны не осталось живых — эпизод терминален (terminated=True).

    Вознаграждение:
        — В процессе: 0 (по умолчанию).
        — Победа BLUE: +1.
        — Победа RED : -1.
    """

    metadata = {"render_modes": []}

    def __init__(self, reward_win: float = 1.0, reward_loss: float = -1.0, reward_step: float = 0.0,
                 log_enabled: bool = False):
        """
        :param reward_win:   награда за победу BLUE
        :param reward_loss:  награда за поражение BLUE
        :param reward_step:  промежуточная награда за шаг (shaping), по умолчанию 0
        :param log_enabled:  включить человекочитаемые логи (используем только на тесте)
        """
        super().__init__()
        # Параметры наград
        self.reward_win  = float(reward_win)
        self.reward_loss = float(reward_loss)
        self.reward_step = float(reward_step)

        # Флаг лога для человекочитаемых сообщений (только для тестового прогона)
        self.log_enabled = bool(log_enabled)

        # Пространство действий: выбор целевой позиции среди 6 красных слотов
        self.action_space = spaces.Discrete(6)

        # Пространство наблюдений: 24 float32 (HP и INI на 12 позициях)
        low  = np.zeros(24, dtype=np.float32)
        high = np.array(sum([[MAX_HP, MAX_INIT] for _ in range(12)], []), dtype=np.float32)
        self.observation_space = spaces.Box(low=low, high=high, dtype=np.float32)

        # ГСЧ для воспроизводимости (инициализируем через seed())
        self.rng = random.Random()

        # Текущее состояние боя
        self.combined = None                   # список из 12 dict-юнитов (копии исходных)
        self.round_no = None                   # номер текущего раунда
        self.winner = None                     # "red" | "blue" | None
        self.current_blue_attacker_pos = None  # позиция синего, который ждёт действие агента

        # Буфер человекочитаемых логов (строки), используем только если log_enabled=True
        self._pretty_events = []

    # ------------------ ВСПОМОГАТЕЛЬНЫЕ МЕТОДЫ ------------------

    def seed(self, seed=None):
        """Установить seed для генератора случайных чисел (воспроизводимость)."""
        self.rng = random.Random(seed)

    def _alive(self, u) -> bool:
        """Вернуть True, если юнит жив (HP > 0)."""
        return u["Здоровье"] > 0

    def _team_alive(self, team: str) -> bool:
        """Есть ли у команды team хотя бы один живой юнит."""
        return any(self._alive(u) and u["team"] == team for u in self.combined)

    def _unit_by_position(self, pos: int):
        """Вернуть словарь-юнит по позиции pos (1..12), либо None, если пусто."""
        return next((u for u in self.combined if u["position"] == pos), None)

    def _live_positions_of(self, team: str):
        """Вернуть список позиций живых юнитов указанной команды."""
        return [u["position"] for u in self.combined if u["team"] == team and self._alive(u)]

    def _log(self, s: str):
        """Добавить строку в человекочитаемый лог (используется при тестовом прогоне)."""
        if self.log_enabled:
            self._pretty_events.append(s)

    def pop_pretty_events(self):
        """
        Забрать накопленные логи и очистить буфер.
        Удобно вызывать сразу после reset()/step(), чтобы получить «кадр» логов.
        """
        out = self._pretty_events[:]
        self._pretty_events.clear()
        return out

    def _reset_state(self):
        """
        Полная инициализация состояния эпизода:
        — Клонируем исходные словари юнитов (чтобы не мутировать глобальные).
        — Устанавливаем текущую инициативу равной базовой.
        — Сбрасываем флаги: номер раунда, победитель, «ожидающий BLUE».
        """
        # У RED заранее нормализовали ключи, у BLUE всё ок
        self.combined = [u.copy() for u in (UNITS_RED + UNITS_BLUE)]
        for u in self.combined:
            u["инициатива"] = u["инициатива_база"]
        self.round_no = 1
        self.winner = None
        self.current_blue_attacker_pos = None
        self._log(f"Эпизод начат. Раунд {self.round_no}.")

    def _candidates(self):
        """
        Вернуть список юнитов, которые могут ходить в текущем раунде:
        — юнит должен быть жив, и
        — его текущая инициатива должна быть > 0.
        """
        return [u for u in self.combined if self._alive(u) and u["инициатива"] > 0]

    def _pop_next(self):
        """
        Выбор следующего ходящего:
        1) Берём кандидатов (живые, INI>0).
        2) Перемешиваем (решаем ничьи по инициативе случайно).
        3) Сортируем по инициативе по убыванию.
        4) Возвращаем верхнего.
        Если кандидатов нет — возвращаем None (конец раунда).
        """
        cand = self._candidates()
        if not cand:
            return None
        self.rng.shuffle(cand)
        cand.sort(key=itemgetter("инициатива"), reverse=True)
        return cand[0]

    def _end_round_restore(self):
        """
        Конец раунда: всем ЖИВЫМ возвращаем инициативу к базовой (инициатива_база),
        мёртвым оставляем 0. Увеличение номера раунда происходит снаружи.
        """
        for u in self.combined:
            u["инициатива"] = u["инициатива_база"] if self._alive(u) else 0
        self._log("Восстановление инициативы. Новый раунд.")

    # ------------------- УДАР И ПРОВЕРКА ПОБЕДЫ -------------------

    def _attack(self, attacker, target_pos: int):
        """
        Выполнить атаку «attacker» по позиции target_pos:
        — Если в целевой позиции есть живой юнит, отнимаем у него HP на величину Урон (20).
        — Если после удара HP <= 0, сбрасываем инициативу жертвы (в этом раунде она уже не сходится).
        — Пишем человекочитаемую запись (если включён лог).
        """
        victim = self._unit_by_position(target_pos) if target_pos is not None else None
        if victim is not None and self._alive(victim):
            before = victim["Здоровье"]
            victim["Здоровье"] -= attacker["Урон"]
            after = victim["Здоровье"]
            self._log(f"{attacker['team'].upper()} {attacker['имя']}#{attacker['position']} → "
                      f"{victim['team'].upper()} {victim['имя']}#{victim['position']}: {attacker['Урон']} "
                      f"({before}→{max(0, after)})")
            if victim["Здоровье"] <= 0:
                victim["инициатива"] = 0
                self._log(f"✖ {victim['team'].upper()} {victim['имя']}#{victim['position']} выведен из строя.")
        else:
            self._log(f"{attacker['team'].upper()} {attacker['имя']}#{attacker['position']} бьёт pos{target_pos}: цели нет/мертва.")

    def _check_victory_after_hit(self):
        """
        Сразу после любого удара проверяем, остались ли живые у обеих сторон.
        При отсутствии живых у одной из сторон — фиксируем победителя и логируем событие.
        """
        if not self._team_alive("blue"):
            self.winner = "red"
            self._log("🏆 Победа RED!")
        elif not self._team_alive("red"):
            self.winner = "blue"
            self._log("🏆 Победа BLUE!")

    # --------------- АВТОДОКРУТКА ДО ХОДА BLUE ----------------

    def _advance_until_blue_turn(self) -> bool:
        """
        Прокручиваем бой автоматически, пока не наступит очередь живого BLUE,
        или пока бой не завершится.
        Возвращает:
            True  — если остановились на ходу BLUE (среда ждёт действие агента),
            False — если бой завершился до этого момента.
        Внутри:
          — Если нет кандидатов на ход → конец раунда: восстановить инициативу и начать новый.
          — Если ход RED → выбрать случайную живую цель среди BLUE и атаковать.
          — После каждого удара — проверка победителя.
        """
        while self._team_alive("red") and self._team_alive("blue"):
            nxt = self._pop_next()
            if nxt is None:
                # Все живые уже сходили — начинаем новый раунд
                self._log(f"— Конец раунда {self.round_no}.")
                self._end_round_restore()
                self.round_no += 1
                self._log(f"— Начало раунда {self.round_no}.")
                continue

            if nxt["team"] == "blue":
                # Пришёл ход живого BLUE — выходим и ждём action от агента
                self.current_blue_attacker_pos = nxt["position"]
                self._log(f"Ход BLUE: {nxt['имя']}#{nxt['position']} (иниц {nxt['инициатива']}). Ожидание действия.")
                return True

            # === Ход RED (противник): случайный выбор живой цели среди BLUE ===
            nxt["инициатива"] = 0  # в рамках текущего раунда этот юнит сходил
            live_blue_positions = self._live_positions_of("blue")
            target_pos = self.rng.choice(live_blue_positions) if live_blue_positions else None
            self._log(f"RED ход: {nxt['имя']}#{nxt['position']} → случайная цель pos{target_pos}.")
            self._attack(nxt, target_pos)
            self._check_victory_after_hit()
            if self.winner is not None:
                return False

        # Боевые действия закончились до очереди BLUE
        return False

    # -------------------- НАБЛЮДЕНИЕ ДЛЯ АГЕНТА --------------------

    def _obs(self) -> np.ndarray:
        """
        Построить наблюдение: плоский вектор (24,) = [HP, INI] для pos=1..12.
        Отрицательные значения (не должны появляться) обрезаем до нуля для стабильности.
        """
        vec = []
        for pos in RED_POSITIONS + BLUE_POSITIONS:
            u = self._unit_by_position(pos)
            vec.extend([float(max(0, u["Здоровье"])), float(max(0, u["инициатива"]))])
        return np.array(vec, dtype=np.float32)

    # ------------------------ API GYMNASIUM ------------------------

    def reset(self, *, seed=None, options=None):
        """
        Сброс эпизода:
          1) Инициализируем состояние боя.
          2) Автоматически докручиваем до первого хода BLUE (или терминала).
          3) Возвращаем начальное наблюдение и пустой info (по стандарту Gymnasium).
        """
        if seed is not None:
            self.seed(seed)
        self._reset_state()
        self._advance_until_blue_turn()
        return self._obs(), {}

    def step(self, action):
        """
        Один шаг среды:
          — Если вдруг сейчас не очередь BLUE, докручиваем до BLUE (или завершаем, если бой уже закончился).
          — Когда очередь BLUE: берём позицию атакующего BLUE и применяем действие агента:
                target_pos = RED_POSITIONS[action]  (удар по одной из 6 красных позиций)
            После удара сразу проверяем победителя.
          — Если победителя нет — снова докручиваем до следующего BLUE (между делом ходят RED по своей политике).
          — Возвращаем (obs, reward, terminated, truncated, info).
        Награда:
          — 0, если бой продолжается;
          — +1, если победил BLUE;
          — -1, если победил RED.
        """
        # Если эпизод уже завершён — просим вызвать reset()
        assert self.winner is None, "Эпизод завершён — вызовите reset()."

        # Подстраховка: если не очередь BLUE, докрутим до его хода
        if self.current_blue_attacker_pos is None:
            self._advance_until_blue_turn()
            if self.winner is not None:
                # Бой завершился ещё до хода BLUE
                return self._obs(), (self.reward_win if self.winner == "blue" else self.reward_loss), True, False, {}

        # === ХОД BLUE (агент) ===
        target_pos = RED_POSITIONS[int(action)]          # перевести индекс действия в реальную позицию
        attacker   = self._unit_by_position(self.current_blue_attacker_pos)
        if attacker is not None and self._alive(attacker) and attacker["инициатива"] > 0:
            self._log(f"BLUE действие: {attacker['имя']}#{attacker['position']} → pos{target_pos}")
            attacker["инициатива"] = 0                   # в этом раунде синий уже сходил
            self._attack(attacker, target_pos)          # применяем урон
            self._check_victory_after_hit()             # проверяем победителя

        # Если бой не завершён — докрутка до следующего BLUE (RED будет ходить автоматически)
        if self.winner is None:
            self.current_blue_attacker_pos = None
            self._advance_until_blue_turn()

        # Финальная сборка перехода
        if self.winner is None:
            reward, terminated = self.reward_step, False
        else:
            reward  = self.reward_win if self.winner == "blue" else self.reward_loss
            terminated = True

        return self._obs(), reward, terminated, False, {}

# =================== ОБУЧЕНИЕ PPO + W&B =====================
if __name__ == "__main__":
    # Импортируем Stable-Baselines3 и колбэки прямо внутри main-блока (чтобы при импорте модуля они не требовались)
    from stable_baselines3 import PPO
    from stable_baselines3.common.env_util import make_vec_env
    from stable_baselines3.common.monitor import Monitor
    from stable_baselines3.common.callbacks import CallbackList, EvalCallback

    # Weights & Biases для трекинга обучения
    import wandb
    from wandb.integration.sb3 import WandbCallback

    # ---------------- ПАРАМЕТРЫ ОБУЧЕНИЯ ----------------
    TOTAL_STEPS   = 300_000   # сколько шагов среды сделает PPO (суммарно по всем векторным копиям)
    N_ENVS        = 8        # сколько параллельных копий среды использовать
    VISUALIZE_TEST = True    # включить ли опциональную визуализацию после теста
    FRAME_DELAY    = 0.28    # задержка между «кадрами» визуализации (сек)
    USE_COLOR      = True    # цветные ANSI в консоли (если терминал поддерживает)

    # ------------- ИНИЦИАЛИЗАЦИЯ W&B ЛОГА --------------
    # Перед запуском сделайте wandb.login() один раз в системе/сессии.
    run = wandb.init(
        project="red-blue-battle",           # поменяйте на ваш проект в W&B
        name=f"ppo-{TOTAL_STEPS//1000}k",    # имя запуска
        config={                             # полезные гиперпараметры для карточки W&B
            "algo": "PPO",
            "total_timesteps": TOTAL_STEPS,
            "n_envs": N_ENVS,
            "n_steps": 1024,
            "batch_size": 2048,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "learning_rate": 3e-4,
            "clip_range": 0.2,
        },
        sync_tensorboard=True,               # синхронизация скаляров TensorBoard → W&B
        save_code=True,                      # прикрепить код к запуску
    )

    # -------- ФАБРИКА СРЕД ДЛЯ ВЕКТОРИЗАЦИИ --------
    def make_env():
        # Оборачиваем среду в Monitor, чтобы SB3 считал эпизодические метрики (ep_rew_mean и т.п.)
        return Monitor(BattleEnv(reward_win=1.0, reward_loss=-1.0, reward_step=0.0, log_enabled=False))

    # Векторная среда: N_ENVS параллельных копий для ускоренного сбора опыта
    vec_env = make_vec_env(make_env, n_envs=N_ENVS, seed=42)

    # Отдельная среда для периодической оценки (EvalCallback)
    eval_env = Monitor(BattleEnv(log_enabled=False))

    # --------------- НАСТРОЙКА МОДЕЛИ PPO ---------------
    model = PPO(
        policy="MlpPolicy",
        env=vec_env,
        verbose=1,                         # печать прогресса обучения SB3
        n_steps=1024,                      # длина rollout на КАЖДУЮ среду (итого N_ENVS * n_steps шагов на обновление)
        batch_size=2048,                   # размер мини-батча SGD (должен делиться на N_ENVS*n_steps / n_epochs)
        gae_lambda=0.95,
        gamma=0.99,
        n_epochs=10,
        learning_rate=3e-4,
        clip_range=0.2,
        ent_coef=0.0,
        vf_coef=0.5,
        seed=42,
        tensorboard_log=f"./tb_logs/{run.id}",  # куда писать TB-логи (W&B их подхватит)
    )

    # Колбэк W&B: сохранение градиентов/модели и синк метрик
    wandb_cb = WandbCallback(
        gradient_save_freq=1000,                     # как часто логировать градиенты (можно убрать для скорости)
        model_save_path=f"./models/{run.id}",        # куда сохранять чекпоинты
        model_save_freq=10_000,                      # период сохранения модели
        verbose=2,
    )

    # Колбэк оценки: периодически замеряем среднюю награду на eval_env
    eval_cb = EvalCallback(
        eval_env,
        best_model_save_path=f"./models/{run.id}/best",
        log_path=f"./eval/{run.id}",
        eval_freq=10_000,               # каждые 10k шагов
        n_eval_episodes=5,
        deterministic=True,
        render=False,
    )

    # Запуск обучения с колбэками (W&B + eval)
    model.learn(total_timesteps=TOTAL_STEPS, callback=CallbackList([wandb_cb, eval_cb]))

    # Сохранение финальной модели
    model.save("ppo_blue_vs_red")
    run.finish()  # корректно закрываем W&B-запуск

    # ================= ТЕСТОВЫЙ ПРОГОН С ЛОГАМИ =================
    print("\n=== ТЕСТОВЫЙ ПРОГОН С ЛОГАМИ ===")
    test_env = BattleEnv(log_enabled=True)      # включаем человекочитаемые логи только на тесте
    obs, info = test_env.reset(seed=123)

    # Собираем все строки логов (будут использованы и для опциональной визуализации)
    all_logs = []
    all_logs += test_env.pop_pretty_events()    # логи, накопленные во время reset()/автодокрутки

    done = False
    total_reward = 0.0
    step_i = 0
    while not done:
        step_i += 1
        action, _ = model.predict(obs, deterministic=True)  # в тесте используем детерминированную политику
        target_pos = RED_POSITIONS[int(action)]
        chosen_line = f"[STEP {step_i}] Агент выбирает action={int(action)} → атака RED pos{target_pos}"
        print("\n" + chosen_line)
        all_logs.append(chosen_line)

        obs, reward, terminated, truncated, info = test_env.step(action)
        total_reward += reward
        done = terminated or truncated

        # Печатаем логи текущего шага и добавляем их в общий список
        new_lines = test_env.pop_pretty_events()
        all_logs += new_lines
        for line in new_lines:
            print(line)

    print("\n=== ЭПИЗОД ЗАВЕРШЁН ===")
    print("Победитель:", test_env.winner.upper(), "| Суммарная награда:", total_reward)

    # ============== ВИЗУАЛИЗАЦИЯ ПО ЛОГАМ (matplotlib, как в checkpointchek) ==============
    # Более наглядная графическая анимация, разбирающая строки логов и рисующая поле боя.
    if VISUALIZE_TEST:
        import math
        import matplotlib.pyplot as plt
        from matplotlib import patches
        from matplotlib.patches import FancyArrowPatch
        try:
            from IPython.display import display
        except Exception:
            display = None

        VISUAL_SPEED_MULT = 8.0

        # --- сетка поля
        RED_FRONT, RED_BACK   = [1,2,3],   [4,5,6]
        BLUE_FRONT, BLUE_BACK = [7,8,9],   [10,11,12]
        COL_X = {0: 0.18, 1: 0.50, 2: 0.82}
        Y_BLUE_BACK, Y_BLUE_FRONT = 0.88, 0.70
        Y_RED_FRONT,  Y_RED_BACK  = 0.30, 0.12
        SLOT_W, SLOT_H = 0.28, 0.13
        HP_H = 0.028

        # --- состояние (имена и стартовые HP)
        state = {u["position"]: {"team": u["team"], "name": u["имя"], "hp": float(u["Здоровье"]), "maxhp": float(u["Здоровье"]) }
                 for u in (UNITS_RED + UNITS_BLUE)}

        fig, ax = plt.subplots(figsize=(12, 8))
        handle = None
        IS_NOTEBOOK = False
        if display is not None:
            try:
                handle = display(fig, display_id=True)
                IS_NOTEBOOK = handle is not None
            except Exception:
                handle = None
                IS_NOTEBOOK = False
        if IS_NOTEBOOK:
            plt.close(fig)
        else:
            plt.ion()
            try:
                fig.show()
            except Exception:
                pass

        def pos_to_xy(pos: int):
            col = (pos - 1) % 3
            x = COL_X[col] - SLOT_W/2
            if pos in BLUE_BACK:   y = Y_BLUE_BACK  - SLOT_H/2
            elif pos in BLUE_FRONT:y = Y_BLUE_FRONT - SLOT_H/2
            elif pos in RED_FRONT: y = Y_RED_FRONT  - SLOT_H/2
            else:                  y = Y_RED_BACK   - SLOT_H/2
            return x, y

        def pos_center(pos: int):
            x, y = pos_to_xy(pos)
            return x + SLOT_W/2, y + SLOT_H/2

        def draw_unit(ax, pos, is_active: bool = False):
            u = state[pos]; x, y = pos_to_xy(pos)
            # Скрываем задний ряд мёртвых (чтобы не загромождать)
            if pos in (RED_BACK + BLUE_BACK) and u["hp"] <= 0:
                return
            card_color = (0.90, 0.30, 0.30, 0.16) if u["team"] == "red" else (0.30, 0.45, 0.90, 0.16)
            ax.add_patch(patches.FancyBboxPatch((x, y), SLOT_W, SLOT_H,
                                                boxstyle="round,pad=0.010,rounding_size=0.016",
                                                linewidth=1.3, edgecolor="black", facecolor=card_color))
            if is_active:
                ax.add_patch(patches.FancyBboxPatch(
                    (x-0.01, y-0.01), SLOT_W+0.02, SLOT_H+0.02,
                    boxstyle="round,pad=0.012,rounding_size=0.018",
                    linewidth=3.0, edgecolor=(1.0, 0.82, 0.10), facecolor='none', alpha=0.95
                ))
            name = u["name"][:22]
            ax.text(x + 0.012, y + SLOT_H*0.78, name,
                    fontsize=10, fontweight="bold", ha="left", va="center", color="black")

            hp_x, hp_y = x + 0.012, y + SLOT_H*0.10
            hp_w = SLOT_W - 0.024
            ax.add_patch(patches.Rectangle((hp_x, hp_y), hp_w, HP_H, facecolor=(0.88,0.88,0.88), edgecolor='none'))
            frac = max(0.0, min(1.0, u["hp"]/u["maxhp"])) if u["maxhp"] > 1e-9 else 0.0
            ax.add_patch(patches.Rectangle((hp_x, hp_y), hp_w*frac, HP_H, facecolor=(0.15,0.70,0.25), edgecolor='none'))
            ax.text(hp_x + hp_w/2, hp_y + HP_H/2, f"{int(max(0,u['hp']))}/{int(u['maxhp'])}",
                    fontsize=9, ha="center", va="center", color="black")

            ax.text(x + SLOT_W/2, y - 0.008, f"pos{pos}", fontsize=8, ha="center", va="top", color=(0.2,0.2,0.2))

        def draw_attack_arrow(ax, src_pos:int, dst_pos:int, team:str,
                              text: str|None = None, style: str = "solid",
                              color: tuple|None = None, alpha: float = 0.95,
                              curve: float = 0.18, lw: float = 2.6):
            sx, sy = pos_center(src_pos)
            dx, dy = pos_center(dst_pos)
            vx, vy = dx - sx, dy - sy
            dist = math.hypot(vx, vy) + 1e-9
            pad = 0.06
            sx += vx/dist * pad; sy += vy/dist * pad
            dx -= vx/dist * pad; dy -= vy/dist * pad
            if color is None:
                color = (0.20, 0.35, 0.85) if team == "BLUE" else (0.85, 0.25, 0.25)
            con_style = f"arc3,rad={curve}" if abs(vx) > 0.01 and abs(vy) > 0.01 else "arc3,rad=0.0"
            arrow = FancyArrowPatch((sx, sy), (dx, dy), arrowstyle="-|>",
                                    mutation_scale=14, linewidth=lw,
                                    linestyle="--" if style=="dashed" else "solid",
                                    color=color, alpha=alpha, connectionstyle=con_style)
            ax.add_patch(arrow)
            if text:
                mx, my = (sx + dx)/2, (sy + dy)/2
                ax.text(mx, my + 0.03, text, fontsize=10, fontweight="bold",
                        ha="center", va="center", color=color)

        # --- регексы (совместимы с текущими логами simpleenv)
        atk_re         = re.compile(r'^(RED|BLUE)\s+[^#]+#(\d+)\s+→\s+(RED|BLUE)\s+[^#]+#(\d+):\s+(\d+)\s+\((\d+)→(\d+)\)')
        kill_re        = re.compile(r'^✖\s+(RED|BLUE)\s+[^#]+#(\d+)\s+выведен из строя\.')
        vict_re        = re.compile(r'^🏆 Победа (RED|BLUE)!')
        blue_turn_re   = re.compile(r'^Ход BLUE:\s+[^#]+#(\d+)')
        red_turn_re    = re.compile(r'^RED ход:\s+[^#]+#(\d+)')
        blue_action_re = re.compile(r'^BLUE действие:\s+[^#]+#(\d+)\s+→\s+pos(\d+)')

        def draw_board(arrows: list[dict] = None, headline: str = "", active_pos: int | None = None):
            ax.cla()
            ax.set_xlim(0, 1); ax.set_ylim(0, 1); ax.axis("off")
            ax.plot([0.02, 0.98], [0.50, 0.50], color=(0.6,0.6,0.6), lw=1.2, ls="--", alpha=0.7)
            ax.text(0.01, 0.96, "BLUE (top)", color=(0.2,0.35,0.8), fontsize=12, fontweight="bold", ha="left")
            ax.text(0.01, 0.04, "RED (bottom)", color=(0.8,0.25,0.25), fontsize=12, fontweight="bold", ha="left")

            arrows = arrows or []
            for pos in (BLUE_BACK + BLUE_FRONT + RED_FRONT + RED_BACK):
                draw_unit(ax, pos, is_active=(active_pos == pos))

            for a in arrows:
                draw_attack_arrow(ax, a["src"], a["dst"], a["team"],
                                  text=a.get("text"), style=a.get("style","solid"),
                                  color=a.get("color"), alpha=a.get("alpha",0.95),
                                  curve=a.get("curve",0.18), lw=a.get("lw",2.6))

            if headline:
                ax.text(0.5, 0.52, headline[:140], fontsize=12, ha="center", va="bottom", color=(0.15,0.15,0.15))

            fig.canvas.draw()
            if handle is not None:
                handle.update(fig)
            else:
                plt.draw(); plt.pause(0.001)

        # --- проигрываем логи покадрово
        current_actor_pos = None
        draw_board(headline="Начало боя", active_pos=current_actor_pos)

        last_selection = None
        for line in all_logs:
            arrows_now = []
            headline = ""

            m = blue_turn_re.match(line)
            if m:
                current_actor_pos = int(m.group(1))
                headline = line
                draw_board(arrows_now, headline, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.05); continue

            m = red_turn_re.match(line)
            if m:
                current_actor_pos = int(m.group(1))
                headline = line
                draw_board(arrows_now, headline, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.05); continue

            m = atk_re.match(line)
            if m:
                atk_team = m.group(1); atk_pos = int(m.group(2))
                vic_pos = int(m.group(4)); dmg = int(m.group(5)); after = int(m.group(7))
                if vic_pos in state: state[vic_pos]["hp"] = after
                current_actor_pos = atk_pos
                arrows_now.append({"src": atk_pos, "dst": vic_pos, "team": atk_team, "text": f"-{dmg}"})
                headline = line
                draw_board(arrows_now, headline, active_pos=current_actor_pos); time.sleep(FRAME_DELAY * VISUAL_SPEED_MULT); continue

            m = blue_action_re.match(line)
            if m:
                src = int(m.group(1)); dst = int(m.group(2))
                last_selection = {"src": src, "dst": dst, "team": "BLUE"}
                current_actor_pos = src
                arrows_now.append({"src": src, "dst": dst, "team": "BLUE",
                                   "style":"dashed", "alpha":0.65, "lw":2.0})
                headline = line
                draw_board(arrows_now, headline, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.1); continue

            m = kill_re.match(line)
            if m:
                pos = int(m.group(2))
                if pos in state: state[pos]["hp"] = 0
                headline = line
                draw_board([], headline, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.1); continue

            if vict_re.match(line) or line.startswith("— ") or "RED ход" in line:
                headline = line
                draw_board([], headline, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.1); continue

            # Прочие строки
            draw_board([], line, active_pos=current_actor_pos); time.sleep((FRAME_DELAY * VISUAL_SPEED_MULT) / 1.2)

        draw_board([], "Конец эпизода", active_pos=None)
